{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis/Source of Confusion: \n",
    "We claim Mutual Information is symmetric. Which is evident from equations and venn diagram -- but not so evident intuitively. \n",
    "\n",
    "\n",
    "\"Reduction in uncertainty about Y(a random variable) after knowing about X(a random variable) is same as reduction in uncertainty about Y after knowing about X.\"  But -- let's say XY is a 2 lettered english word. I tell you that the second letter is \"O\", would the amount of uncertainty reduction in the first letter be same as compared to when I tell that the first letter is \"N\". Intuitively the uncertainty reduction in the second case should be higher as compared the first case (as the number of 2 lettered words which start with \"N\" is very less as compared to the number of words that end with \"O\"). So how does symmetry of mutual information hold true here? \n",
    "\n",
    "## Problem statement : \n",
    "If we have a 2 lettered word. Then we need to show that \"knowing\" about one letter reduces the uncertainity in the second letter same as the uncertainity reduction in the first letter when we know the second letter.\n",
    "\n",
    "## Method:\n",
    "- Fetched all meaningful 2 lettered english words.\n",
    "- Created a joint prob distribution assuming they have equal occurrence (first letter as rv X, second letter as rv Y)\n",
    "- Calculated the marginal entropy, joint entropy, conditional entropy ,mutual information\n",
    "- Proved our initial intuition that they aren't independent (as a side result!)\n",
    "- Also realised that the difference in the --  \"uncertainty assuming indpependent\" - \"actual uncertaintiy\" = mutual information. In hindsight -- very obvious from venn diagram but still :P\n",
    "- Observed that uncertainty reduces with evidence \"on average\" (as a side result!)\n",
    "- By implementing this, it got clear that I should start thinking in terms of probabilities and expectation.\n",
    "\n",
    "## Learning:\n",
    "Learnt the hard way that when we say \"knowing\" we mean knowing a probability distribution and not the individual value sampled from the distribution. The conditional entropy always reduces, Mutual informatoon is symmetric -- these statements hold true \"on average\". Thus, its not about the uncertainty about the first letter after we know \"O\" -- but the uncertainity in the first letter after we are given the prob distribution of the second letter. This means we will average the \"indidual conditional probabilitties for each value of the second letter\" and then substract it from the original uncertainity on the first letter. Even though individually the case with \"N\" as first letter has the max mutual information H(Y|X=\"N\") = 0 but this is just one term -- you need to calculate the average sum of such individual conditional entropies (weighed by th prob of occurrence of second letter) and then substract from the original uncertainty of the second letter. TLDR; I finally realised that I need to substract the average across the entire probability distribution weighted by the probability of their occurrence (so this accounts not just for \"N\" but for \"A-Z\" every element from the set of X). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import json\n",
    "import copy\n",
    "from spellchecker import SpellChecker\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#surprisingly there were a lot of weird words in the corpus.\n",
    "def clean_word_list(wrd_lst):\n",
    "    cp_lst = copy.deepcopy(wrd_lst)\n",
    "    spell = SpellChecker()\n",
    "    misspelled = spell.unknown(wrd_lst)\n",
    "    for wrd in misspelled:\n",
    "        cp_lst.remove(wrd)\n",
    "    return cp_lst \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217\n"
     ]
    }
   ],
   "source": [
    "## Download from here: https://github.com/dwyl/english-words/blob/master/words_dictionary.json\n",
    "filepath = '/home/ayhaos/Downloads/words_dictionary.json'\n",
    "with open(filepath, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "word_list = set(data.keys())\n",
    "wrd_lst = set(w.lower() for w in word_list if len(w) == 2)\n",
    "final_list = clean_word_list(wrd_lst)\n",
    "print(len(final_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_matrix = np.zeros((26,26), dtype=int) # the sample set  of X and Y both are entire english alphabet\n",
    "legend = {alpha:i for i,alpha in enumerate(list(string.ascii_lowercase))}\n",
    "for wrd in wrd_lst:\n",
    "    freq_matrix[legend[wrd[0]]][legend[wrd[1]]] = 1\n",
    "    \n",
    "#normalize to get word prob (we assume equal word occurence)\n",
    "prob_matrix = freq_matrix / freq_matrix.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginal Entropy of X: 4.6227\n",
      "Marginal Entropy of Y: 4.5881\n",
      "\n",
      "Expected Uncertainity in Y after observing X: 4.1154\n",
      "Expected Uncertainity in X after observing Y: 4.1500\n",
      "\n",
      "Correct Joint Entropy: 8.7381\n",
      "Joint Entropy if we assumed both independent: 9.2108\n",
      "Mutual Information as a delta of joint entropy of independent - dependent: 0.4727\n",
      "\n",
      "Mutual Information as expected reduction in uncertainity of Y given we observe X:0.4727\n",
      "Mutual Information as expected reduction in uncertainity of X given we observe Y: 0.4727\n"
     ]
    }
   ],
   "source": [
    "#entropy H(X|Y) \n",
    "P_X = np.sum(prob_matrix, axis=1) #marginal of X -- over all possible Y\n",
    "P_Y = np.sum(prob_matrix, axis=0) #marginal of Y -- over all possible X\n",
    "\n",
    "\n",
    "def P_Y_given_X(prob_matrix, idx):\n",
    "    #Bayes Theorem\n",
    "    #what if the prob of X=x_i is 0\n",
    "    if np.sum(prob_matrix, axis=1)[idx] == 0:\n",
    "        return 1 # to make log (1) = 0 so that sum doesn't get affected\n",
    "    return prob_matrix[idx] / np.sum(prob_matrix, axis=1)[idx]\n",
    "\n",
    "def P_X_given_Y(prob_matrix, idx):\n",
    "    #Bayes Theorem\n",
    "    #what if the prob of Y=y_i is 0\n",
    "    if np.sum(prob_matrix, axis=0)[idx] == 0:\n",
    "        return 1 # to make log (1) = 0 so that sum doesn't get affected\n",
    "    return prob_matrix[:,idx] / np.sum(prob_matrix, axis=0)[idx]\n",
    "\n",
    "\n",
    "H_X = -1 * np.sum(P_X*np.log2(P_X))# summation p(x).log(p(x))\n",
    "H_Y = -1 * np.sum(P_Y*np.log2(P_Y))# summation p(y).log(p(y))\n",
    "\n",
    "print(f'Marginal Entropy of X: {H_X:.4f}')\n",
    "print(f'Marginal Entropy of Y: {H_Y:.4f}\\n')\n",
    "\n",
    "##summation over x,y of p(x,y)log(1/p(y|x)) \n",
    "H_Y_given_X =  0\n",
    "for (x,y),val in np.ndenumerate(prob_matrix):\n",
    "    if val != 0:\n",
    "        H_Y_given_X += val*np.log2(P_Y_given_X(prob_matrix, x)[y])\n",
    "\n",
    "##summation over x,y of p(x,y)log(1/p(x|y)) \n",
    "H_X_given_Y = 0\n",
    "for (x,y),val in np.ndenumerate(prob_matrix):\n",
    "    if val != 0:\n",
    "        H_X_given_Y += val*np.log2(P_X_given_Y(prob_matrix, y)[x])\n",
    "\n",
    "H_Y_given_X = -1* H_Y_given_X\n",
    "H_X_given_Y = -1* H_X_given_Y\n",
    "print(f'Expected Uncertainity in Y after observing X: {H_Y_given_X:.4f}') #the uncertainity always reduces with additional evidence (On Average!)\n",
    "print(f'Expected Uncertainity in X after observing Y: {H_X_given_Y:.4f}\\n') #the uncertainity always reduces with additional evidence (On Average!)\n",
    "\n",
    "#Joint Entropy\n",
    "H_XY = H_X + H_Y_given_X \n",
    "\n",
    "#Joint Entropy if we assume them independent \n",
    "H_XY_wrong = H_X + H_Y\n",
    "\n",
    "#the difference proves that they aren't independent\n",
    "print(f'Correct Joint Entropy: {H_XY:.4f}')\n",
    "print(f'Joint Entropy if we assumed both independent: {H_XY_wrong:.4f}')\n",
    "print(f'Mutual Information as a delta of joint entropy of independent - dependent: {H_XY_wrong-H_XY:.4f}\\n')\n",
    "\n",
    "#mutual information\n",
    "I_XY = H_Y - H_Y_given_X\n",
    "print(f'Mutual Information as expected reduction in uncertainity of Y given we observe X:{I_XY:.4f}')\n",
    "\n",
    "#let's see if symmetric\n",
    "I_XY2 = H_X - H_X_given_Y\n",
    "print(f'Mutual Information as expected reduction in uncertainity of X given we observe Y: {I_XY2:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai2thor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
