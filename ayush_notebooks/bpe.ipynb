{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character level BPE (Byte Pair Encoding) with word boundaries\n",
    "\n",
    "Instead of using the regular alphabet we wish to find better set of symbols (character sequences that appear togetehr frequently) and treat them as single units.\n",
    "\n",
    "### Intuition:\n",
    "\n",
    "If \"at\" appears 200 times in your text then why represent it using 'a' and 't' separately. Instead make [at] a new symbol.\n",
    "\n",
    "### learnings:\n",
    "\n",
    "- implemented char level BPE (an algorithm similar to the tokenizer for GPT-2. It uses Byte level BPE with no explicit word endings)\n",
    "- For our model we need to ensure that \"cat btb tb\" is not parsed as {'c':1, 'a':1, 'tb' : 3} i.e. word endings are to be taken seriously.\n",
    "- for this split words at space and added a '_' to mark end\n",
    "- then the frequency counter was run on each word basis and the most frequent across the whole text was merged in vocabulary\n",
    "- as the number of merges (each merge is 1 iteration updating the vocabulary) increases -- we are able to capture more symbols and thus reduce the tokens in the test text encodding.\n",
    "- once we have trained (generated the vocabulary on the basis of statistics) we can encode any given text. the compression varies depenind on the distribution match between the train text and testing text.\n",
    "- should be noted that character level restricts us to english only (the language of training)\n",
    "- byte level would make this language independent (what GPT2 and modern LLMs do). They can even tokenize emojis!\n",
    "\n",
    "\n",
    "### Overall:\n",
    "\n",
    "BPE discovers linguistic structure automatically just by statistics!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "BPE EXAMPLE: \"aaabcaabbd\" for 1 merge\n",
    "\n",
    "Input: \"aaabcaabbd_\"\n",
    "Tokens: ['a','a','a','b','c','a','a','b','b','d','_']  (11 tokens)\n",
    "\n",
    "Count adjacent pairs:\n",
    "  ('a','a'): 3  ← MOST FREQUENT\n",
    "  ('a','b'): 2\n",
    "  ('b','c'): 1\n",
    "  ('b','b'): 1\n",
    "  ('b','d'): 1\n",
    "  ('d','_'): 1\n",
    "\n",
    "Merge most frequent: ('a','a') → 'aa'\n",
    "\n",
    "Output: ['aa','a','b','c','aa','b','b','d','_']  (9 tokens)\n",
    "\n",
    "Compression: 11 → 9 tokens\n",
    "\n",
    "\n",
    "### For text with words and spaces\n",
    "\n",
    "BPE in 4 Steps:\n",
    "\n",
    "1. Add word boundaries:     \"the cat\" → [\"the_\", \"cat_\"]\n",
    "\n",
    "2. Find most frequent pair:  ('t','h') appears 100x\n",
    "\n",
    "3. Merge everywhere:         ('t','h','e','_') → ('th','e','_')\n",
    "\n",
    "4. Repeat N times → Learn N new symbols\n",
    "\n",
    "Result: Learned vocabulary of subword units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/yanranzhao/n-gram-language-identification/master/en-the-little-prince.txt\"\n",
    "raw_text = requests.get(url).text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean text and add word boundary markers.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = re.sub(r'([.,!?;:\\'\\\"-])', r' \\1 ', text)\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    words = text.split()\n",
    "    return [word + '_' for word in words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def get_vocab(words):\n",
    "    \"\"\"\n",
    "    Get vocabulary from words.\n",
    "    Each word is represented as a tuple of tokens.\n",
    "    \n",
    "    words: list of strings like ['the_', 'cat_']\n",
    "    returns: dict mapping word tuple to frequency\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for word in words:\n",
    "        # Each word starts as tuple of characters\n",
    "        word_tuple = tuple(word)\n",
    "        vocab[word_tuple] = vocab.get(word_tuple, 0) + 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def get_pairs(vocab):\n",
    "    \"\"\"\n",
    "    Count all adjacent pairs across all words in vocab.\n",
    "    \n",
    "    vocab: {('t','h','e','_'): 5, ('c','a','t','_'): 3}\n",
    "    returns: Counter of pairs\n",
    "    \"\"\"\n",
    "    pairs = Counter()\n",
    "    \n",
    "    for word, freq in vocab.items():\n",
    "        for i in range(len(word) - 1):\n",
    "            pairs[(word[i], word[i+1])] += freq\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "\n",
    "def merge_vocab(pair, vocab):\n",
    "    \"\"\"\n",
    "    Merge all occurrences of pair in vocab.\n",
    "    \n",
    "    pair: ('t', 'h')\n",
    "    vocab: {('t','h','e','_'): 5} \n",
    "    returns: {('th','e','_'): 5}\n",
    "    \"\"\"\n",
    "    new_vocab = {}\n",
    "    \n",
    "    # Create the merged token\n",
    "    merged = ''.join(pair)\n",
    "    \n",
    "    for word, freq in vocab.items():\n",
    "        # Merge pair in this word\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            if i < len(word) - 1 and word[i] == pair[0] and word[i+1] == pair[1]:\n",
    "                new_word.append(merged)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        \n",
    "        new_vocab[tuple(new_word)] = freq\n",
    "    \n",
    "    return new_vocab\n",
    "\n",
    "\n",
    "def train_bpe(words, num_merges, verbose=True):\n",
    "    \"\"\"\n",
    "    Train BPE.\n",
    "    \n",
    "    words: list of strings ['the_', 'cat_', ...]\n",
    "    num_merges: number of merge operations\n",
    "    returns: list of merge operations [(pair, merged_token), ...]\n",
    "    \"\"\"\n",
    "    # Get initial vocabulary (character level)\n",
    "    vocab = get_vocab(words)\n",
    "    \n",
    "    merges = []\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Training on {len(words)} words\")\n",
    "        print(f\"Unique words: {len(vocab)}\")\n",
    "        print(f\"Learning {num_merges} merges...\\n\")\n",
    "    \n",
    "    for i in range(num_merges):\n",
    "        pairs = get_pairs(vocab)\n",
    "        \n",
    "        if not pairs:\n",
    "            break\n",
    "        \n",
    "        # Most frequent pair\n",
    "        best_pair = max(pairs, key=pairs.get)\n",
    "        freq = pairs[best_pair]\n",
    "        \n",
    "        if freq < 2:\n",
    "            break\n",
    "        \n",
    "        # Merge it\n",
    "        vocab = merge_vocab(best_pair, vocab)\n",
    "        merged_token = ''.join(best_pair)\n",
    "        merges.append((best_pair, merged_token))\n",
    "        \n",
    "        if verbose and (i < 5 or i >= num_merges - 3):\n",
    "            print(f\"Merge {i+1}: {best_pair} → '{merged_token}' (freq: {freq})\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nLearned {len(merges)} merges\")\n",
    "    \n",
    "    return merges\n",
    "\n",
    "\n",
    "def encode_word(word, merges):\n",
    "    \"\"\"\n",
    "    Apply BPE merges to a single word.\n",
    "    \n",
    "    word: string like 'running_'\n",
    "    merges: learned merge operations\n",
    "    returns: list of tokens ['r', 'u', 'nn', 'ing_']\n",
    "    \"\"\"\n",
    "    # Start with characters\n",
    "    tokens = list(word)\n",
    "    \n",
    "    # Apply each merge in order\n",
    "    for pair, merged in merges:\n",
    "        i = 0\n",
    "        new_tokens = []\n",
    "        while i < len(tokens):\n",
    "            if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n",
    "                new_tokens.append(merged)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "        tokens = new_tokens\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def encode_text(text, merges):\n",
    "    \"\"\"\n",
    "    Tokenize text using learned BPE merges.\n",
    "    \n",
    "    text: raw string\n",
    "    merges: learned merge operations\n",
    "    returns: list of tokens\n",
    "    \"\"\"\n",
    "    words = preprocess_text(text)\n",
    "    \n",
    "    all_tokens = []\n",
    "    for word in words:\n",
    "        word_tokens = encode_word(word, merges)\n",
    "        all_tokens.extend(word_tokens)\n",
    "    \n",
    "    return all_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1705 words\n",
      "Unique words: 477\n",
      "Learning 500 merges...\n",
      "\n",
      "Merge 1: ('e', '_') → 'e_' (freq: 243)\n",
      "Merge 2: ('t', 'h') → 'th' (freq: 170)\n",
      "Merge 3: ('t', '_') → 't_' (freq: 150)\n",
      "Merge 4: ('d', '_') → 'd_' (freq: 138)\n",
      "Merge 5: ('s', '_') → 's_' (freq: 136)\n",
      "Merge 498: ('m', 'or') → 'mor' (freq: 2)\n",
      "Merge 499: ('mor', 'e_') → 'more_' (freq: 2)\n",
      "Merge 500: ('l', 'at') → 'lat' (freq: 2)\n",
      "\n",
      "Learned 500 merges\n",
      "We have learnt frequency based symbols by 500 merges.\n",
      "\n",
      "\n",
      "======================================================================\n",
      "LEARNED MERGES\n",
      "======================================================================\n",
      " 1. ('e', '_') → 'e_'\n",
      " 2. ('t', 'h') → 'th'\n",
      " 3. ('t', '_') → 't_'\n",
      " 4. ('d', '_') → 'd_'\n",
      " 5. ('s', '_') → 's_'\n",
      " 6. ('n', '_') → 'n_'\n",
      " 7. ('e', 'r') → 'er'\n",
      " 8. ('y', '_') → 'y_'\n",
      " 9. ('.', '_') → '._'\n",
      "10. ('i', 'n') → 'in'\n",
      "11. ('a', 'n') → 'an'\n",
      "12. (',', '_') → ',_'\n",
      "13. ('i', '_') → 'i_'\n",
      "14. ('a', '_') → 'a_'\n",
      "15. ('o', '_') → 'o_'\n",
      "16. ('th', 'e_') → 'the_'\n",
      "17. ('o', 'u') → 'ou'\n",
      "18. ('e', 'd_') → 'ed_'\n",
      "19. ('f', '_') → 'f_'\n",
      "20. ('in', 'g') → 'ing'\n",
      "21. ('r', 'e') → 're'\n",
      "22. ('h', 'a') → 'ha'\n",
      "23. ('i', 's_') → 'is_'\n",
      "24. ('er', '_') → 'er_'\n",
      "25. ('ing', '_') → 'ing_'\n",
      "26. ('\"', '_') → '\"_'\n",
      "27. ('r', 'a') → 'ra'\n",
      "28. ('o', 'r') → 'or'\n",
      "29. ('an', 'd_') → 'and_'\n",
      "30. ('t', 'o_') → 'to_'\n",
      "31. ('o', 'n') → 'on'\n",
      "32. ('o', 'f_') → 'of_'\n",
      "33. ('s', 't') → 'st'\n",
      "34. ('a', 't_') → 'at_'\n",
      "35. ('e', 'n') → 'en'\n",
      "36. ('a', 'l') → 'al'\n",
      "37. ('e', 'e') → 'ee'\n",
      "38. ('m', 'y_') → 'my_'\n",
      "39. ('a', 'r') → 'ar'\n",
      "40. ('m', '_') → 'm_'\n",
      "41. ('r', 'o') → 'ro'\n",
      "42. ('w', 'a') → 'wa'\n",
      "43. ('i', 'c') → 'ic'\n",
      "44. ('d', 'ra') → 'dra'\n",
      "45. ('dra', 'w') → 'draw'\n",
      "46. ('-', '_') → '-_'\n",
      "47. ('or', '_') → 'or_'\n",
      "48. ('i', 't_') → 'it_'\n",
      "49. ('l', 'i') → 'li'\n",
      "50. ('o', 'n_') → 'on_'\n",
      "51. ('w', 'h') → 'wh'\n",
      "52. ('m', 'a') → 'ma'\n",
      "53. ('p', '_') → 'p_'\n",
      "54. ('th', 'e') → 'the'\n",
      "55. ('s', 'h') → 'sh'\n",
      "56. ('v', 'e_') → 've_'\n",
      "57. ('m', 'e_') → 'me_'\n",
      "58. ('wa', 's_') → 'was_'\n",
      "59. ('th', 'at_') → 'that_'\n",
      "60. ('l', 'e_') → 'le_'\n",
      "61. ('l', 'e') → 'le'\n",
      "62. ('l', '_') → 'l_'\n",
      "63. ('l', 'y_') → 'ly_'\n",
      "64. ('i', 'n_') → 'in_'\n",
      "65. (':', '_') → ':_'\n",
      "66. ('b', 'o') → 'bo'\n",
      "67. ('l', 'd_') → 'ld_'\n",
      "68. ('th', 'is_') → 'this_'\n",
      "69. ('s', 'i') → 'si'\n",
      "70. ('k', '_') → 'k_'\n",
      "71. ('a', 't') → 'at'\n",
      "72. ('ha', 've_') → 'have_'\n",
      "73. ('o', 'w') → 'ow'\n",
      "74. ('draw', 'ing_') → 'drawing_'\n",
      "75. ('ee', 'p_') → 'eep_'\n",
      "76. ('c', 'e_') → 'ce_'\n",
      "77. ('c', 'h') → 'ch'\n",
      "78. ('w', 'i') → 'wi'\n",
      "79. ('h', 'e_') → 'he_'\n",
      "80. ('e', 'v') → 'ev'\n",
      "81. ('g', 'h') → 'gh'\n",
      "82. ('ic', 't') → 'ict'\n",
      "83. ('th', '_') → 'th_'\n",
      "84. ('a', 's') → 'as'\n",
      "85. ('e', 'n_') → 'en_'\n",
      "86. ('c', 'on') → 'con'\n",
      "87. ('on', 'e_') → 'one_'\n",
      "88. ('d', 'e_') → 'de_'\n",
      "89. ('s', 'e') → 'se'\n",
      "90. ('sh', 'eep_') → 'sheep_'\n",
      "91. ('s', 'a') → 'sa'\n",
      "92. ('er', 'y_') → 'ery_'\n",
      "93. ('t', 't') → 'tt'\n",
      "94. ('g', 'ro') → 'gro'\n",
      "95. ('gro', 'w') → 'grow'\n",
      "96. ('grow', 'n_') → 'grown_'\n",
      "97. ('f', 'ro') → 'fro'\n",
      "98. ('fro', 'm_') → 'from_'\n",
      "99. ('s', 'o_') → 'so_'\n",
      "100. ('a', 'n_') → 'an_'\n",
      "101. ('d', 'e') → 'de'\n",
      "102. ('r', 'i') → 'ri'\n",
      "103. ('u', 'n') → 'un'\n",
      "104. ('a', 'b') → 'ab'\n",
      "105. ('ou', 't_') → 'out_'\n",
      "106. ('er', 'e_') → 'ere_'\n",
      "107. ('al', 'l_') → 'all_'\n",
      "108. ('n', 'o') → 'no'\n",
      "109. ('i', 'on_') → 'ion_'\n",
      "110. ('y', 'ou') → 'you'\n",
      "111. ('e', 'x') → 'ex'\n",
      "112. ('s', 't_') → 'st_'\n",
      "113. ('no', 't_') → 'not_'\n",
      "114. ('m', 'e') → 'me'\n",
      "115. ('u', 'r') → 'ur'\n",
      "116. ('st', 'r') → 'str'\n",
      "117. ('d', 'i') → 'di'\n",
      "118. ('v', 'ery_') → 'very_'\n",
      "119. ('u', 'p') → 'up'\n",
      "120. ('li', 'tt') → 'litt'\n",
      "121. ('litt', 'le_') → 'little_'\n",
      "122. ('th', 'er_') → 'ther_'\n",
      "123. ('s', 'e_') → 'se_'\n",
      "124. ('g', 're') → 'gre'\n",
      "125. ('m', 'b') → 'mb'\n",
      "126. ('bo', 'a_') → 'boa_'\n",
      "127. ('con', 'str') → 'constr'\n",
      "128. ('constr', 'ict') → 'constrict'\n",
      "129. ('wi', 'th_') → 'with_'\n",
      "130. ('l', 'o') → 'lo'\n",
      "131. ('ou', 'ld_') → 'ould_'\n",
      "132. ('an', 'y_') → 'any_'\n",
      "133. ('si', 'de_') → 'side_'\n",
      "134. ('h', 'i') → 'hi'\n",
      "135. ('m', 'i') → 'mi'\n",
      "136. ('ha', 'd_') → 'had_'\n",
      "137. ('you', '_') → 'you_'\n",
      "138. ('th', 'ing_') → 'thing_'\n",
      "139. ('i', 'r') → 'ir'\n",
      "140. ('e', 's_') → 'es_'\n",
      "141. ('up', 's_') → 'ups_'\n",
      "142. ('en', 't_') → 'ent_'\n",
      "143. ('the', 'n_') → 'then_'\n",
      "144. ('k', 'ed_') → 'ked_'\n",
      "145. ('b', 'u') → 'bu'\n",
      "146. ('bu', 't_') → 'but_'\n",
      "147. ('a', 'in') → 'ain'\n",
      "148. ('f', 'or_') → 'for_'\n",
      "149. ('w', '_') → 'w_'\n",
      "150. ('the', 'm_') → 'them_'\n",
      "151. ('mb', 'er_') → 'mber_'\n",
      "152. ('t', 'er_') → 'ter_'\n",
      "153. ('ur', 'e_') → 'ure_'\n",
      "154. ('sa', 'i') → 'sai'\n",
      "155. ('ow', '_') → 'ow_'\n",
      "156. ('n', 'e') → 'ne'\n",
      "157. ('g', 'e') → 'ge'\n",
      "158. ('s', 'o') → 'so'\n",
      "159. ('ha', 't_') → 'hat_'\n",
      "160. ('t', 'i') → 'ti'\n",
      "161. ('g', 'ra') → 'gra'\n",
      "162. ('ev', 'er_') → 'ever_'\n",
      "163. ('hi', 'm_') → 'him_'\n",
      "164. ('draw', '_') → 'draw_'\n",
      "165. ('t', 'o') → 'to'\n",
      "166. ('u', 'l') → 'ul'\n",
      "167. ('ch', 'i') → 'chi'\n",
      "168. ('d', 're') → 'dre'\n",
      "169. ('o', 'k_') → 'ok_'\n",
      "170. ('u', 'p_') → 'up_'\n",
      "171. ('d', 'er') → 'der'\n",
      "172. ('i', 'f_') → 'if_'\n",
      "173. ('wh', 'en_') → 'when_'\n",
      "174. ('x', '_') → 'x_'\n",
      "175. ('e', 'ar') → 'ear'\n",
      "176. ('constrict', 'or_') → 'constrictor_'\n",
      "177. ('sai', 'd_') → 'said_'\n",
      "178. ('o', 'v') → 'ov'\n",
      "179. ('s', 'u') → 'su'\n",
      "180. ('b', 'e_') → 'be_'\n",
      "181. ('p', 'h') → 'ph'\n",
      "182. ('ee', 'n_') → 'een_'\n",
      "183. ('c', 'a') → 'ca'\n",
      "184. ('a', 's_') → 'as_'\n",
      "185. ('gre', 'at_') → 'great_'\n",
      "186. ('w', 'ould_') → 'would_'\n",
      "187. ('i', 't') → 'it'\n",
      "188. ('!', '_') → '!_'\n",
      "189. ('s', 'er') → 'ser'\n",
      "190. ('b', 'e') → 'be'\n",
      "191. ('f', 'ri') → 'fri'\n",
      "192. ('an', 'o') → 'ano'\n",
      "193. ('ano', 'ther_') → 'another_'\n",
      "194. ('un', 'der') → 'under'\n",
      "195. ('y', 'thing_') → 'ything_'\n",
      "196. ('ab', 'out_') → 'about_'\n",
      "197. ('ou', 'gh') → 'ough'\n",
      "198. ('ough', '_') → 'ough_'\n",
      "199. ('at', 'ion_') → 'ation_'\n",
      "200. ('si', 'x_') → 'six_'\n",
      "201. ('p', 'ict') → 'pict'\n",
      "202. ('al', '_') → 'al_'\n",
      "203. ('s', 'w') → 'sw'\n",
      "204. ('the', 'y_') → 'they_'\n",
      "205. ('ab', 'le_') → 'able_'\n",
      "206. ('ov', 'er_') → 'over_'\n",
      "207. ('n', 'u') → 'nu'\n",
      "208. ('nu', 'mber_') → 'number_'\n",
      "209. ('k', 'e_') → 'ke_'\n",
      "210. ('?', '_') → '?_'\n",
      "211. ('b', 'y_') → 'by_'\n",
      "212. ('an', 't_') → 'ant_'\n",
      "213. ('in', 'side_') → 'inside_'\n",
      "214. ('p', 'l') → 'pl'\n",
      "215. ('se', 'l') → 'sel'\n",
      "216. ('e', 'a') → 'ea'\n",
      "217. ('gh', 't_') → 'ght_'\n",
      "218. ('b', 'een_') → 'been_'\n",
      "219. ('ma', 'n_') → 'man_'\n",
      "220. ('s', 'and_') → 'sand_'\n",
      "221. ('s', 'm') → 'sm'\n",
      "222. ('sm', 'all_') → 'small_'\n",
      "223. ('en', 'ce_') → 'ence_'\n",
      "224. ('chi', 'l') → 'chil'\n",
      "225. ('chil', 'dre') → 'childre'\n",
      "226. ('childre', 'n_') → 'children_'\n",
      "227. ('wh', 'o_') → 'who_'\n",
      "228. ('bo', 'ok_') → 'book_'\n",
      "229. ('re', 'as') → 'reas'\n",
      "230. ('under', 'st') → 'underst'\n",
      "231. ('c', 'o') → 'co'\n",
      "232. ('ar', 'e_') → 'are_'\n",
      "233. ('wi', 'l') → 'wil'\n",
      "234. ('wil', 'l_') → 'will_'\n",
      "235. ('c', 'ha') → 'cha'\n",
      "236. ('n', 'i') → 'ni'\n",
      "237. ('pict', 'ure_') → 'picture_'\n",
      "238. ('t', 'r') → 'tr'\n",
      "239. ('u', 'e_') → 'ue_'\n",
      "240. ('a', 'c') → 'ac'\n",
      "241. ('s', 'l') → 'sl'\n",
      "242. ('en', 't') → 'ent'\n",
      "243. ('k', 'ing_') → 'king_'\n",
      "244. ('lo', 'o') → 'loo'\n",
      "245. ('loo', 'ked_') → 'looked_'\n",
      "246. ('gh', 't') → 'ght'\n",
      "247. ('en', 'ed_') → 'ened_'\n",
      "248. ('ma', 'de_') → 'made_'\n",
      "249. ('se', 'e_') → 'see_'\n",
      "250. ('le', 'ar') → 'lear'\n",
      "251. ('al', 'wa') → 'alwa'\n",
      "252. ('alwa', 'y') → 'alway'\n",
      "253. ('alway', 's_') → 'always_'\n",
      "254. (\"'\", '_') → ''_'\n",
      "255. ('m', 'y') → 'my'\n",
      "256. ('sel', 'f_') → 'self_'\n",
      "257. ('g', 'e_') → 'ge_'\n",
      "258. ('w', 'hat_') → 'what_'\n",
      "259. ('ca', 're') → 'care'\n",
      "260. ('f', 'a') → 'fa'\n",
      "261. ('n', 'ever_') → 'never_'\n",
      "262. ('ch', '_') → 'ch_'\n",
      "263. ('ou', 'n') → 'oun'\n",
      "264. ('er', 's_') → 'ers_'\n",
      "265. ('q', 'u') → 'qu'\n",
      "266. ('g', '_') → 'g_'\n",
      "267. ('m', 'p') → 'mp'\n",
      "268. ('p', 'er') → 'per'\n",
      "269. ('st', 'ar') → 'star'\n",
      "270. ('th', 'ou') → 'thou'\n",
      "271. ('thou', 'sand_') → 'thousand_'\n",
      "272. ('mi', 'le') → 'mile'\n",
      "273. ('mile', 's_') → 'miles_'\n",
      "274. ('ha', 'b') → 'hab'\n",
      "275. ('hab', 'it') → 'habit'\n",
      "276. ('s', 's_') → 'ss_'\n",
      "277. ('n', 'o_') → 'no_'\n",
      "278. ('r', 'in') → 'rin'\n",
      "279. ('re', 'a') → 'rea'\n",
      "280. ('de', 'd') → 'ded'\n",
      "281. ('ded', 'ic') → 'dedic'\n",
      "282. ('reas', 'on_') → 'reason_'\n",
      "283. ('w', 'or') → 'wor'\n",
      "284. ('an', 'd') → 'and'\n",
      "285. ('ev', 'er') → 'ever'\n",
      "286. ('ever', 'ything_') → 'everything_'\n",
      "287. ('li', 'v') → 'liv'\n",
      "288. ('an', 'ce_') → 'ance_'\n",
      "289. ('wh', 'ere_') → 'where_'\n",
      "290. ('un', 'g') → 'ung'\n",
      "291. ('en', 'ough_') → 'enough_'\n",
      "292. ('wh', 'o') → 'who'\n",
      "293. ('w', 'ere_') → 'were_'\n",
      "294. ('on', 'ce_') → 'once_'\n",
      "295. ('f', 'e') → 'fe'\n",
      "296. ('re', 'me') → 'reme'\n",
      "297. ('y', 'ear') → 'year'\n",
      "298. ('year', 's_') → 'years_'\n",
      "299. ('o', 'ld_') → 'old_'\n",
      "300. ('ma', 'g') → 'mag'\n",
      "301. ('f', 'ic') → 'fic'\n",
      "302. ('al', 'l') → 'all'\n",
      "303. ('tr', 'ue_') → 'true_'\n",
      "304. ('st', 'or') → 'stor'\n",
      "305. ('p', 'ri') → 'pri'\n",
      "306. ('f', 'o') → 'fo'\n",
      "307. ('fo', 're') → 'fore'\n",
      "308. ('h', 'ere_') → 'here_'\n",
      "309. ('constrict', 'or') → 'constrictor'\n",
      "310. ('constrictor', 's_') → 'constrictors_'\n",
      "311. ('m', 'o') → 'mo'\n",
      "312. ('sl', 'eep_') → 'sleep_'\n",
      "313. ('m', 'on') → 'mon'\n",
      "314. ('ne', 'ed_') → 'need_'\n",
      "315. ('ge', 'st') → 'gest'\n",
      "316. ('so', 'me_') → 'some_'\n",
      "317. ('ir', 'st_') → 'irst_'\n",
      "318. ('li', 'ke_') → 'like_'\n",
      "319. ('fri', 'ght') → 'fright'\n",
      "320. ('an', 'sw') → 'answ'\n",
      "321. ('er', 'ed_') → 'ered_'\n",
      "322. ('wh', 'y_') → 'why_'\n",
      "323. ('e', 'le') → 'ele'\n",
      "324. ('ele', 'ph') → 'eleph'\n",
      "325. ('eleph', 'ant_') → 'elephant_'\n",
      "326. ('ing', 's_') → 'ings_'\n",
      "327. ('ex', 'pl') → 'expl'\n",
      "328. ('t', 'w') → 'tw'\n",
      "329. ('tw', 'o_') → 'two_'\n",
      "330. ('s', 'p') → 'sp'\n",
      "331. ('ti', 'me_') → 'time_'\n",
      "332. ('ou', 't') → 'out'\n",
      "333. ('out', 'side_') → 'outside_'\n",
      "334. ('my', 'self_') → 'myself_'\n",
      "335. ('ea', 'd_') → 'ead_'\n",
      "336. ('ge', 'o') → 'geo'\n",
      "337. ('geo', 'gra') → 'geogra'\n",
      "338. ('geogra', 'ph') → 'geograph'\n",
      "339. ('geograph', 'y_') → 'geography_'\n",
      "340. ('ar', 'i') → 'ari'\n",
      "341. ('t', 'ic') → 'tic'\n",
      "342. ('n', 'ed_') → 'ned_'\n",
      "343. ('p', 'ar') → 'par'\n",
      "344. ('ha', 's_') → 'has_'\n",
      "345. ('d', 'ge_') → 'dge_'\n",
      "346. ('li', 'f') → 'lif'\n",
      "347. ('lif', 'e_') → 'life_'\n",
      "348. ('c', 'er') → 'cer'\n",
      "349. ('ma', 'tt') → 'matt'\n",
      "350. ('t', 'e') → 'te'\n",
      "351. ('s', 'ee') → 'see'\n",
      "352. ('m', 'ent_') → 'ment_'\n",
      "353. ('per', 's') → 'pers'\n",
      "354. ('pers', 'on_') → 'person_'\n",
      "355. ('t', 'al') → 'tal'\n",
      "356. ('tal', 'k_') → 'talk_'\n",
      "357. ('h', 'is_') → 'his_'\n",
      "358. ('p', 'le') → 'ple'\n",
      "359. ('ple', 'as') → 'pleas'\n",
      "360. ('de', 'ser') → 'deser'\n",
      "361. ('deser', 't_') → 'desert_'\n",
      "362. ('en', 'g') → 'eng'\n",
      "363. ('in', 'e_') → 'ine_'\n",
      "364. ('c', 'e') → 'ce'\n",
      "365. ('h', 'u') → 'hu'\n",
      "366. ('hu', 'man_') → 'human_'\n",
      "367. ('habit', 'ation_') → 'habitation_'\n",
      "368. ('d', 'd') → 'dd'\n",
      "369. ('j', 'u') → 'ju'\n",
      "370. ('e', 't_') → 'et_'\n",
      "371. ('to', 'o_') → 'too_'\n",
      "372. ('h', 'ow_') → 'how_'\n",
      "373. ('t', 'ed_') → 'ted_'\n",
      "374. ('li', 've_') → 'live_'\n",
      "375. ('p', 'rin') → 'prin'\n",
      "376. ('prin', 'ce_') → 'prince_'\n",
      "377. ('le', 'on_') → 'leon_'\n",
      "378. ('w', 'er') → 'wer'\n",
      "379. ('wer', 'th_') → 'werth_'\n",
      "380. ('in', 'd') → 'ind'\n",
      "381. ('ind', 'ul') → 'indul'\n",
      "382. ('indul', 'g') → 'indulg'\n",
      "383. ('ma', 'y_') → 'may_'\n",
      "384. ('dedic', 'at') → 'dedicat'\n",
      "385. ('ser', 'i') → 'seri'\n",
      "386. ('seri', 'ou') → 'seriou'\n",
      "387. ('be', 'st_') → 'best_'\n",
      "388. ('fri', 'en') → 'frien'\n",
      "389. ('frien', 'd_') → 'friend_'\n",
      "390. ('wor', 'ld_') → 'world_'\n",
      "391. ('underst', 'and') → 'understand'\n",
      "392. ('h', 'ung') → 'hung'\n",
      "393. ('r', 'y_') → 'ry_'\n",
      "394. ('ee', 'd') → 'eed'\n",
      "395. ('ch', 'e') → 'che'\n",
      "396. ('er', 'ing_') → 'ering_'\n",
      "397. ('chi', 'ld_') → 'child_'\n",
      "398. ('reme', 'mber_') → 'remember_'\n",
      "399. ('re', 'c') → 'rec'\n",
      "400. ('cha', 'p') → 'chap'\n",
      "401. ('chap', 'ter_') → 'chapter_'\n",
      "402. ('sa', 'w_') → 'saw_'\n",
      "403. ('mag', 'ni') → 'magni'\n",
      "404. ('magni', 'fic') → 'magnific'\n",
      "405. ('magnific', 'ent_') → 'magnificent_'\n",
      "406. ('at', 'ure_') → 'ature_'\n",
      "407. ('pri', 'm') → 'prim'\n",
      "408. ('prim', 'ev') → 'primev'\n",
      "409. ('primev', 'al_') → 'primeval_'\n",
      "410. ('sw', 'all') → 'swall'\n",
      "411. ('ow', 'ing_') → 'owing_'\n",
      "412. ('wi', 'th') → 'with'\n",
      "413. ('with', 'out_') → 'without_'\n",
      "414. ('a', 'f') → 'af'\n",
      "415. ('af', 'ter_') → 'after_'\n",
      "416. ('di', 'gest') → 'digest'\n",
      "417. ('a', 'd') → 'ad'\n",
      "418. ('ad', 'v') → 'adv'\n",
      "419. ('u', 're') → 'ure'\n",
      "420. ('ure', 's_') → 'ures_'\n",
      "421. ('en', 'c') → 'enc'\n",
      "422. ('f', 'irst_') → 'first_'\n",
      "423. ('so', 'me') → 'some'\n",
      "424. ('some', 'thing_') → 'something_'\n",
      "425. ('er', 'p') → 'erp'\n",
      "426. ('as', 'ked_') → 'asked_'\n",
      "427. ('wh', 'e') → 'whe'\n",
      "428. ('whe', 'ther_') → 'whether_'\n",
      "429. ('fright', 'ened_') → 'frightened_'\n",
      "430. ('answ', 'ered_') → 'answered_'\n",
      "431. ('underst', 'and_') → 'understand_'\n",
      "432. ('dre', 'w_') → 'drew_'\n",
      "433. ('c', 'ould_') → 'could_'\n",
      "434. ('c', 'lear') → 'clear'\n",
      "435. ('th', 'ings_') → 'things_'\n",
      "436. ('expl', 'ain') → 'explain'\n",
      "437. ('hi', 'stor') → 'histor'\n",
      "438. ('histor', 'y_') → 'history_'\n",
      "439. ('ari', 'th') → 'arith'\n",
      "440. ('arith', 'me') → 'arithme'\n",
      "441. ('arithme', 'tic') → 'arithmetic'\n",
      "442. ('arithmetic', '_') → 'arithmetic_'\n",
      "443. ('gra', 'm') → 'gram'\n",
      "444. ('gram', 'm') → 'gramm'\n",
      "445. ('gramm', 'ar') → 'grammar'\n",
      "446. ('grammar', '_') → 'grammar_'\n",
      "447. ('g', 'a') → 'ga'\n",
      "448. ('ga', 've_') → 'gave_'\n",
      "449. ('mi', 'ght_') → 'might_'\n",
      "450. ('care', 'er_') → 'career_'\n",
      "451. ('p', 'ain') → 'pain'\n",
      "452. ('pain', 'ter_') → 'painter_'\n",
      "453. ('an', 'ything_') → 'anything_'\n",
      "454. ('lear', 'ned_') → 'learned_'\n",
      "455. ('a', 'ir') → 'air'\n",
      "456. ('pl', 'an') → 'plan'\n",
      "457. ('ow', 'n_') → 'own_'\n",
      "458. ('t', 's_') → 'ts_'\n",
      "459. ('c', 'an_') → 'can_'\n",
      "460. ('i', 'sh') → 'ish'\n",
      "461. ('lo', 'st_') → 'lost_'\n",
      "462. ('ni', 'ght_') → 'night_'\n",
      "463. ('su', 'ch_') → 'such_'\n",
      "464. ('k', 'n') → 'kn'\n",
      "465. ('c', 'ou') → 'cou'\n",
      "466. ('m', 'any_') → 'many_'\n",
      "467. ('oun', 't') → 'ount'\n",
      "468. ('p', 'e') → 'pe'\n",
      "469. ('o', 'p') → 'op'\n",
      "470. ('con', 'se') → 'conse'\n",
      "471. ('conse', 'qu') → 'consequ'\n",
      "472. ('consequ', 'ence_') → 'consequence_'\n",
      "473. ('liv', 'ed_') → 'lived_'\n",
      "474. ('de', 'al_') → 'deal_'\n",
      "475. ('a', 'mon') → 'amon'\n",
      "476. ('amon', 'g_') → 'among_'\n",
      "477. ('te', 'ly_') → 'tely_'\n",
      "478. ('s', 'n_') → 'sn_'\n",
      "479. ('m', 'u') → 'mu'\n",
      "480. ('mu', 'ch_') → 'much_'\n",
      "481. ('me', 't_') → 'met_'\n",
      "482. ('see', 'm') → 'seem'\n",
      "483. ('seem', 'ed_') → 'seemed_'\n",
      "484. ('p', 't_') → 'pt_'\n",
      "485. ('p', 'o') → 'po'\n",
      "486. ('c', 'k') → 'ck'\n",
      "487. ('al', 'one_') → 'alone_'\n",
      "488. ('ha', 'r') → 'har'\n",
      "489. ('eng', 'ine_') → 'engine_'\n",
      "490. ('ne', 'i') → 'nei'\n",
      "491. ('nei', 'ther_') → 'neither_'\n",
      "492. ('n', 'or_') → 'nor_'\n",
      "493. ('ul', 't_') → 'ult_'\n",
      "494. ('de', 'a') → 'dea'\n",
      "495. ('dea', 'th_') → 'death_'\n",
      "496. ('l', 'as') → 'las'\n",
      "497. ('las', 't_') → 'last_'\n",
      "498. ('m', 'or') → 'mor'\n",
      "499. ('mor', 'e_') → 'more_'\n",
      "500. ('l', 'at') → 'lat'\n",
      "We can now encode any given new text\n",
      "Original Length: 2296 characters\n",
      "Original Word count: 417 words\n",
      "New Tokens: ['the_', 'little_', 'prince_', 'said_', 'that_', 'the_', 'little_', 'fo', 'x_', 'to', 'ld_', 'the_', 'little_', 'prince_', 'about_', 'the_', 'ro', 'se_', ',_', 'and_', 'the_', 'ro', 'se_', 'was_', 'sp', 'e', 'c', 'i', 'al_', 'be', 'ca', 'u', 'se_', 'the_', 'little_', 'prince_', 'had_', 't', 'a', 'm', 'ed_', 'the_', 'ro', 'se_', ',_', 'and_', 't', 'a', 'm', 'ing_', 'me', 'an', 's_', 'c', 're', 'at', 'ing_', 'ti', 'es_', ',_', 'and_', 'the_', 'little_', 'prince_', 'underst', 'o', 'o', 'd_', 'that_', 'the_', 'ro', 'se_', 'was_', 'un', 'i', 'q', 'ue_', 'among_', 'all_', 'the_', 'ro', 'se', 's_', 'in_', 'the_', 'world_', 'be', 'ca', 'u', 'se_', 'the_', 'little_', 'prince_', 'had_', 'sp', 'ent_', 'time_', 'with_', 'the_', 'ro', 'se_', ',_', 'wa', 't', 'ering_', 'the_', 'ro', 'se_', 'and_', 'p', 'ro', 'te', 'c', 't', 'ing_', 'the_', 'ro', 'se_', 'from_', 'the_', 'w', 'in', 'd_', ',_', 'and_', 'the_', 'little_', 'prince_', 're', 'al', 'i', 'z', 'ed_', 'that_', 'what_', 'ma', 'k', 'es_', 'the_', 'desert_', 'be', 'a', 'u', 'ti', 'f', 'u', 'l_', 'is_', 'that_', 'some', 'where_', 'it_', 'hi', 'de', 's_', 'a_', 'w', 'e', 'l', 'l_', ',_', 'and_', 'what_', 'ma', 'k', 'es_', 'the_', 'star', 's_', 'be', 'a', 'u', 'ti', 'f', 'u', 'l_', 'is_', 'the_', 'in', 'v', 'i', 'si', 'b', 'le_', 'f', 'l', 'ow', 'er_', 'that_', 'b', 'loo', 'm', 's_', 'some', 'where_', 'among_', 'them_', ',_', 'and_', 'the_', 'little_', 'prince_', 'learned_', 'that_', 'it_', 'is_', 'on', 'ly_', 'with_', 'the_', 'h', 'ear', 't_', 'that_', 'one_', 'can_', 'see_', 'ri', 'ght', 'ly_', ',_', 'be', 'ca', 'u', 'se_', 'what_', 'is_', 'e', 's', 's', 'en', 'ti', 'al_', 'is_', 'in', 'v', 'i', 'si', 'b', 'le_', 'to_', 'the_', 'e', 'y', 'e_', ',_', 'and_', 'the_', 'fo', 'x_', 'said_', 'to_', 'the_', 'little_', 'prince_', 'that_', 'the_', 'time_', 'the_', 'little_', 'prince_', 'had_', 'sp', 'ent_', 'with_', 'the_', 'ro', 'se_', 'made_', 'the_', 'ro', 'se_', 'i', 'mp', 'or', 't', 'ant_', ',_', 'and_', 'the_', 'little_', 'prince_', 'would_', 'fore', 'v', 'er_', 'be_', 're', 'sp', 'on', 'si', 'b', 'le_', 'for_', 'the_', 'ro', 'se_', 'be', 'ca', 'u', 'se_', 'the_', 'little_', 'prince_', 'had_', 't', 'a', 'm', 'ed_', 'the_', 'ro', 'se_', ',_', 'and_', 'the_', 'b', 'a', 'o', 'b', 'ab', 's_', 'mu', 'st_', 'be_', 'p', 'ul', 'l', 'ed_', 'up_', 'as_', 'so', 'on_', 'as_', 'they_', 'can_', 'be_', 'di', 'st', 'ing', 'u', 'ish', 'ed_', 'from_', 'the_', 'ro', 'se', 'bu', 'sh', 'es_', ',_', 'be', 'ca', 'u', 'se_', 'if_', 'the_', 'b', 'a', 'o', 'b', 'ab', 's_', 'grow', '_', 'too_', 'l', 'ar', 'ge_', 'they_', 'will_', 'de', 'st', 'ro', 'y_', 'the_', 'little_', 'prince_', \"'_\", 's_', 'plan', 'et_', ',_', 'and_', 'the_', 'little_', 'prince_', 'v', 'i', 'si', 'ted_', 'many_', 'plan', 'e', 'ts_', 'in', 'c', 'l', 'u', 'd', 'ing_', 'the_', 'plan', 'et_', 'of_', 'the_', 'king_', 'who_', 'c', 'l', 'a', 'i', 'm', 'ed_', 'to_', 'r', 'u', 'le_', 'over_', 'everything_', ',_', 'and_', 'the_', 'plan', 'et_', 'of_', 'the_', 'con', 'ce', 'it', 'ed_', 'man_', 'who_', 'w', 'an', 'ted_', 'ever', 'y', 'one_', 'to_', 'ad', 'mi', 'r', 'e_', 'him_', ',_', 'and_', 'the_', 'plan', 'et_', 'of_', 'the_', 'd', 'r', 'un', 'k', 'ar', 'd_', 'who_', 'd', 'r', 'an', 'k_', 'to_', 'f', 'or', 'ge', 't_', 'that_', 'he_', 'was_', 'as', 'ha', 'm', 'ed_', 'of_', 'd', 'rin', 'king_', ',_', 'and_', 'the_', 'plan', 'et_', 'of_', 'the_', 'bu', 's', 'in', 'e', 's', 's', 'man_', 'who_', 'c', 'oun', 'ted_', 'the_', 'star', 's_', 'and_', 'c', 'l', 'a', 'i', 'm', 'ed_', 'to_', 'own_', 'them_', ',_', 'and_', 'the_', 'plan', 'et_', 'of_', 'the_', 'l', 'a', 'mp', 'li', 'gh', 'ter_', 'who_', 'fa', 'i', 'th', 'f', 'ul', 'ly_', 'l', 'it_', 'and_', 'ex', 't', 'ing', 'u', 'ish', 'ed_', 'his_', 'l', 'a', 'm', 'p_', 'ev', 'en_', 'th', 'ough_', 'his_', 'plan', 'et_', 'sp', 'u', 'n_', 'fa', 'st', 'er_', 'and_', 'fa', 'st', 'er_', ',_', 'and_', 'the_', 'n', 'ar', 'ra', 't', 'or_', 'met_', 'the_', 'little_', 'prince_', 'in_', 'the_', 'desert_', 'when_', 'the_', 'n', 'ar', 'ra', 't', 'or_', \"'_\", 's_', 'plan', 'e_', 'c', 'ra', 'sh', 'ed_', ',_', 'and_', 'the_', 'n', 'ar', 'ra', 't', 'or_', 'drew_', 'pict', 'ures_', 'for_', 'the_', 'little_', 'prince_', 'in', 'c', 'l', 'u', 'd', 'ing_', 'a_', 'sheep_', 'in_', 'a_', 'bo', 'x_', 'be', 'ca', 'u', 'se_', 'the_', 'little_', 'prince_', 'w', 'an', 'ted_', 'a_', 'sheep_', 'to_', 'e', 'at_', 'the_', 'b', 'a', 'o', 'b', 'ab', 's_', 'but_', 'not_', 'to_', 'e', 'at_', 'the_', 'ro', 'se_', 'with_', 'th', 'or', 'n', 's_', ',_', 'and_', 'the_', 'little_', 'prince_', 'asked_', 'the_', 'n', 'ar', 'ra', 't', 'or_', 'to_', 'draw_', 'a_', 'mu', 'z', 'z', 'le_', 'for_', 'the_', 'sheep_', 'so_', 'the_', 'sheep_', 'could_', 'not_', 'e', 'at_', 'the_', 'ro', 'se_', ',_', 'and_', 'ev', 'ent', 'u', 'al', 'ly_', 'the_', 'little_', 'prince_', 're', 't', 'ur', 'ned_', 'to_', 'his_', 'plan', 'et_', 'by_', 'all', 'owing_', 'the_', 's', 'n', 'a', 'ke_', 'to_', 'b', 'it', 'e_', 'him_', ',_', 'and_', 'the_', 'n', 'ar', 'ra', 't', 'or_', 'st', 'i', 'l', 'l_', 'loo', 'k', 's_', 'up_', 'at_', 'the_', 'star', 's_', 'w', 'on', 'der', 'ing_', 'if_', 'the_', 'sheep_', 'has_', 'e', 'at', 'en_', 'the_', 'ro', 'se_', 'or_', 'if_', 'the_', 'ro', 'se_', 'is_', 'sa', 'f', 'e_', 'be', 'h', 'in', 'd_', 'h', 'er_', 'g', 'las', 's_', 'g', 'lo', 'be_', ',_', 'and_', 'the_', 'n', 'ar', 'ra', 't', 'or_', 'te', 'l', 'l', 's_', 'u', 's_', 'that_', 'all_', 'grown_', '-_', 'ups_', 'were_', 'once_', 'children_', 'but_', 'fe', 'w_', 'of_', 'them_', 'remember_', 'it_', '._']\n",
      "Compression: 2296 chars → 800 tokens\n",
      "\n",
      "Compression: 65.2%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "words = preprocess_text(raw_text)\n",
    "merges = train_bpe(words, num_merges=500, verbose=True)\n",
    "print(f\"We have learnt frequency based symbols by {len(merges)} merges.\\n\")\n",
    "\n",
    "# Show learned vocabulary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LEARNED MERGES\")\n",
    "print(\"=\" * 70)\n",
    "for i, (pair, merged) in enumerate(merges, 1):\n",
    "    print(f\"{i:2d}. {pair} → '{merged}'\")\n",
    "    \n",
    "test_text = \"\"\"\n",
    "The little prince said that the little fox told the little prince about the rose, and the rose was special because the little prince had tamed the rose, and taming means creating ties, and the little prince understood that the rose was unique among all the roses in the world because the little prince had spent time with the rose, watering the rose and protecting the rose from the wind, and the little prince realized that what makes the desert beautiful is that somewhere it hides a well, and what makes the stars beautiful is the invisible flower that blooms somewhere among them, and the little prince learned that it is only with the heart that one can see rightly, because what is essential is invisible to the eye, and the fox said to the little prince that the time the little prince had spent with the rose made the rose important, and the little prince would forever be responsible for the rose because the little prince had tamed the rose, and the baobabs must be pulled up as soon as they can be distinguished from the rosebushes, because if the baobabs grow too large they will destroy the little prince's planet, and the little prince visited many planets including the planet of the king who claimed to rule over everything, and the planet of the conceited man who wanted everyone to admire him, and the planet of the drunkard who drank to forget that he was ashamed of drinking, and the planet of the businessman who counted the stars and claimed to own them, and the planet of the lamplighter who faithfully lit and extinguished his lamp even though his planet spun faster and faster, and the narrator met the little prince in the desert when the narrator's plane crashed, and the narrator drew pictures for the little prince including a sheep in a box because the little prince wanted a sheep to eat the baobabs but not to eat the rose with thorns, and the little prince asked the narrator to draw a muzzle for the sheep so the sheep could not eat the rose, and eventually the little prince returned to his planet by allowing the snake to bite him, and the narrator still looks up at the stars wondering if the sheep has eaten the rose or if the rose is safe behind her glass globe, and the narrator tells us that all grown-ups were once children but few of them remember it.\n",
    "\"\"\"\n",
    "\n",
    "print(\"We can now encode any given new text\")\n",
    "tokens = encode_text(test_text, merges)\n",
    "\n",
    "print(f\"Original Length: {len(test_text)} characters\")\n",
    "print(f\"Original Word count: {len(test_text.split())} words\")\n",
    "\n",
    "print(f\"New Tokens: {tokens}\")\n",
    "print(f\"Compression: {len(test_text)} chars → {len(tokens)} tokens\\n\")\n",
    "print(f\"Compression: {(1 - len(tokens)/len(test_text))*100:.1f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai2thor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
